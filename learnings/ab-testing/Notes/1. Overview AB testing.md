>[@vkteam][https://vkteam.medium.com/practitioners-guide-to-statistical-tests-ed2d580ef04f#1e3b], [@roadmap_sh][https://roadmap.sh/ai-data-scientist]
### High-level overview:
- A/B testing involves in determine whether a change has a meaningful impact.
- A/B testing comparing two groups - Group A (control) and Group B (treatment) to see if their outcomes differ significantly.
- **The choice of statistical test matters because it affects how accurately you detect true differences (sensitivity) and how well you handle messy real-world data (robustness).**
### Concepts:
1. **A/B Testing**:
    - **Definition**: A method to compare two versions of something (e.g., a website feature or algorithm) by splitting users into two groups: Group A (control, no change) and Group B (treatment, with the change). You measure outcomes (e.g., clicks, time spent) to see if the change makes a difference.
    - **In the Article**: A/B testing is the context for statistical tests, used to evaluate recommender system tweaks.
2. **Statistical Test**:
    - **Definition**: A mathematical tool to determine if the difference between two groups (e.g., A and B) is real or just due to random chance. It calculates a _p-value_ to guide decisions.
    - **In the Article**: The authors compare tests like t-tests, Mann-Whitney U, bootstrap, and permutation tests to find the best ones for A/B testing.
3. **Null Hypothesis (H₀)**:
    - **Definition**: The assumption that there’s no difference between groups A and B (e.g., the new algorithm doesn’t improve outcomes). Statistical tests try to reject this hypothesis.
    - **In the Article**: Tests aim to reject H₀ if the data suggests a real difference (e.g., Group B performs better).
4. **Alternative Hypothesis (H₁)**:
    - **Definition**: The opposite of the null hypothesis, suggesting there _is_ a difference (e.g., the new algorithm works better).
    - **In the Article**: A low p-value supports H₁, indicating the change had an effect.
5. **P-Value**:
    - **Definition**: A number showing the probability of seeing the observed results (or more extreme) if the null hypothesis is true. A small p-value (e.g., less than 0.05) suggests the difference is unlikely due to chance, so you reject H₀.
    - **In the Article**: P-values help decide if Group B’s performance is significantly better.
6. **Type I Error (False Positive Rate, α)**:
    - **Definition**: Mistakenly rejecting the null hypothesis when it’s true (e.g., saying the new algorithm works when it doesn’t). Typically, α = 0.05 means a 5% chance of this error.
    - **In the Article**: The authors ensure tests maintain a low Type I error rate to avoid false conclusions.
7. **Type II Error (False Negative, β)**:
    - **Definition**: Failing to reject the null hypothesis when it’s false (e.g., missing a real improvement). The _power_ of a test (1 - β) measures its ability to avoid this error.
    - **In the Article**: Tests are evaluated on power to ensure they detect true effects.
8. **Power**:
    - **Definition**: The probability a test correctly rejects the null hypothesis when there’s a real difference (1 - β). Higher power means a test is better at spotting true effects.
    - **In the Article**: Simulations compare how powerful each test is across data types.
9. **Data Distribution**:
    - **Definition**: The shape of the data, describing how values spread out. Common types include:
        - **Normal Distribution**: Bell-shaped, symmetric (e.g., heights of people).
        - **Skewed Distribution**: Lopsided, with a long tail (e.g., time spent on a website).
        - **Heavy-Tailed Distribution**: Has extreme values (outliers) more often (e.g., revenue per user).
    - **In the Article**: Tests are evaluated on different distributions because real-world data varies.
10. **Outliers**:
    - **Definition**: Extreme values far from the rest of the data (e.g., one user spending $10,000 when most spend $10).
    - **In the Article**: Some tests (like t-tests) struggle with outliers, while others (like Mann-Whitney U) handle them better.
11. **Statistical Tests Evaluated**: The article examines several tests, each with specific assumptions and strengths:
    - **T-Test**: Assumes normal data, compares means. Variants include:
        - **Student’s T-Test**: Equal variances between groups.
        - **Welch’s T-Test**: Unequal variances, more robust.
    - **Mann-Whitney U Test**: Non-parametric, compares distributions without assuming normality. Good for skewed data.
    - **Bootstrap**: Resamples data to estimate differences. Flexible but slow.
    - **Permutation Test**: Shuffles group labels to test differences. Robust for small samples.
    - **Z-Test**: Like a t-test but for large samples.
    - **Others**: Like Kolmogorov-Smirnov (compares distributions) or log-transformed tests for skewed data.
12. **Sensitivity**:
    - **Definition**: A test’s ability to detect small but real differences (high power).
    - **In the Article**: Tests are compared on sensitivity to ensure they catch meaningful changes.
13. **Robustness**:
    - **Definition**: A test’s ability to give reliable results despite data issues (e.g., outliers, non-normal distributions).
    - **In the Article**: Robust tests like Mann-Whitney U or bootstrap are favored for messy data.
14. **Effect Size**:
    - **Definition**: A measure of how big the difference is between groups (e.g., Group B’s average is 10% higher). Examples include Cohen’s d or mean difference.
    - **In the Article**: Simulations vary effect sizes to test how well tests detect them.
15. **Sample Size**:
    - **Definition**: The number of observations (e.g., users) in each group. Larger samples improve test reliability.
    - **In the Article**: Tests are evaluated across small and large samples, as small samples can be tricky.

### Key Points from the Article
- **Why Tests Matter**: Choosing the wrong test can lead to missing real effects (low power) or false positives (wrong conclusions). For example, t-tests assume normality, but recommender system data (e.g., watch time) is often skewed or has outliers.
- **Simulation Approach**: The authors run thousands of simulated A/B tests with different data distributions (normal, log-normal, heavy-tailed) and effect sizes. They measure:
    - **Type I Error Rate**: Does the test keep false positives near 5% (α = 0.05)?
    - **Power**: How often does the test detect a true difference?
- **Findings**:
    - **T-Tests**: Work well for normal data but fail with outliers or skewness. Welch’s t-test is better for unequal variances.
    - **Mann-Whitney U**: Robust for non-normal data, good for skewed distributions, but less powerful for normal data.
    - **Bootstrap**: Handles complex data well but is computationally heavy, making it slow for large datasets.
    - **Permutation Tests**: Great for small samples or non-standard metrics but slow for big data.
    - **Log-Transformation**: Applying a log function to skewed data can make t-tests viable.
- **Trade-Offs**: No test is perfect. Sensitive tests (high power) may overreact to outliers, while robust tests may miss subtle effects.
- **Visualizations**: Animated charts show how test performance changes with data type, sample size, and effect size, making results intuitive.
- **Recommendations**:
    - Use **Welch’s t-test** for roughly normal data or large samples.
    - Use **Mann-Whitney U** for skewed or outlier-heavy data.
    - Consider **bootstrap** or **permutation tests** for small samples or complex metrics.
    - Pre-process data (e.g., log-transform) if it helps meet test assumptions.