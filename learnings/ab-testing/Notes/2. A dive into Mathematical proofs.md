### A dive into Mathematical proofs
Some definitions before diving in:
- **Degree of Freedom:** the number of independent observations in a sample of data that are available to estimate a parameter of the population from which that sample is drawn. (You imagine it as sample size)
- **Standard deviation:** The distance between 1 point to the mean in the sample (_=square root of variance_)
- **Variance:** How the data spread across distributions (_average of the square difference from the average_)
- **T-Distribution:** Tail heaviness is determined by a parameter of the t-distribution called degrees of freedom, with smaller values giving heavier tails, and with higher values making the t-distribution resemble a standard normal distribution with a mean of 0 and a standard deviation of 1. (Also called Student's t-distribution)
- **CDF:** Cumulative distribution function
- **Note on p-value:**  If p > 0.05 and p ≤ 0.1, it means that there will be a low assumption for the null hypothesis. **If p > 0.01 and p ≤ 0.05, then there must be a strong assumption about the null hypothesis**.
##### Mathematical proofs:
1. **Student's T-Test**
	For two independent samples $X1,X2,…,Xn_1$ (control) and $Y1,Y2,…,Yn_2$ (treatment):
	$$t = \frac{\overline{X} - \overline{Y}}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$
	Where:
	- $\overline{X}, \overline{Y}$: sample means ($\overline{X} = \frac{1}{n_1} \sum{X_i}, \overline{Y} = \frac{1}{n_2} \sum{Y_i}$)
	- $s_p$: pooled standard deviation:$$ s_p = \sqrt{\frac{(n_1-1)s^2_X + (n_2 - 1)S_Y^2}{n_1+n_2-2}}$$
	- $s_X^2, s_Y^2$: Sample variances $(S^2_X = \frac{1}{n_1-1}\sum(X_i-\overline{X}^2))$
	- $n_1, n_2$: Sample sizes
	- Degrees of freedom: $df = n_1 + n_2 - 2$
	**P-Value**: Compare $t$ to the t-distribution with $df$ to compute the probability of observing a test statistic as extreme under the null hypothesis $(H0:μX=μY).$
	**Derivation Insight:**
	- assume $X_i \sim N(\mu_X, σ^2), Y_i ~ N(\mu_Y, σ^2)$ (same variance)
	- The difference $\overline{X} - \overline{Y} \sim N(\mu_X - \mu_Y, σ^2(\frac{1}{n_1} + \frac{1}{n_2}))$ 
	- The pooled variance $s^2_p$ estimates $σ^2$, and the resulting statistic follows a t-distribution under $H_0$
	**Proof Node:** The t-distribution arises because $s_p^2$  is an estimate, introducing variability (unlike a z-test with known $\sigma^2$).

2. **Welch's T-Test**
	**Purpose:** Like Student's t-test but allows unequal variances
	**Formula:** $$t=\frac{\overline{X} - \overline{Y}}{\sqrt{\frac{s^2_X}{n_1} + \frac{s^2_y}{n_2}}}$$
	Where:
	- $\overline{X}, \overline{Y}, s^2_X, s^2_Y, n_1, n_2$: As above.
	- Degrees of freedom (approximated via Welch-Satterthwaite): $$df = \frac{(\frac{s^2_X}{n_1}+\frac{s^2_Y}{n_2})^2}{\frac{(s^2_X/n_1)^2}{n_1-1} + \frac{(s^2_Y/n_2)^2}{n_2-1}}$$
	**P-value:** Compare $t$ to the t-distribution with $df$
	**Derivation Insight:**
	- Assume $X_i \sim N(\mu_X, \sigma_X^2), Y_i \sim N(\mu_Y, \sigma_Y^2)$ (different variances)
	- The variance of the difference is $Var(\overline{X}-\overline{Y}) = Var(\overline{X}) + Var(\overline{Y}) = \frac{\sigma^2}{n_1} + \frac{\sigma^2}{n_2}$
	- Estimate with sample variances: $\frac{s^2_X}{n_1} + \frac{s_Y^2}{n_2}$
	- The test statistic is approximately t-distributed, but $df$ is adjusted to account for unequal variances
	**Proof Note**: The Welch-Satterthwaite approximation ensures the statistic’s distribution is close to a t-distribution, avoiding the equal-variance assumption.
	
3. **Mann-Whitney U Test**
	**Purpose:** Non-parametric test to compare whether one group's values tend to be larger than the other's
	**Formula:**
	For sample $X_1, ..., X_n, Y_1, ..., Y_n$
	 - Combine all observations and rank them (smallest = 1, largest = $n_1 + n_2$)
	 - Compute the sum of ranks for each group: $R_X$ (control), $R_Y$ (treatment).
	 - Calculate: $$U_X = n_1n_2 + \frac{n_1(n_1+1)}{2} - R_X$$ $$U_Y = n_1n_2 + \frac{n_1(n_1+1)}{2} - R_Y$$
	 - Use $U = min(U_X, U_Y)$
	 - For large samples, approximate with a z-score: $$z=\frac{U-\mu_u}{\sigma_U}$$
	 Where: $$\mu_U = \frac{n_1n_2}{2}, \sigma_U=\sqrt{\frac{n_1n_2(n_1+n_2+1)}{12}}$$
	 (adjust $\sigma_U$ for ties if present)
	 **P-Value**: For small samples, use exact U-tables; for large samples, use the normal approximation.
	 **Derivation Insight**:
	- Under $H_0$: Distributions are identical, so ranks are equally likely across groups.
	- UX U_X UX​ counts how many times $X_i > Y_j$ (adjusted for ties).
	- The expected value and variance of $U$ under $H_0$ lead to the z-score approximation.
	- The test compares the rank sums, focusing on stochastic dominance rather than means.
	**Proof Note**: The normal approximation holds for large $n_1, n_2$ due to the Central Limit Theorem applied to rank sums.
	
4. **Bootstrap Test**
	**Purpose**: Estimates the distribution of a statistic (e.g., mean difference) by resampling data.
	**Formula/Algorithm**:
	- Observed statistic: $\theta = \bar{Y} - \bar{X}$
	- For $B$ bootstrap iterations:
		1. Combine data: $Z = \{X_1, \dots, X_{n_1}, Y_1, \dots, Y_{n_2}\}.$
		2. Draw a sample of size $n_1 + n_2$ with replacement: $Z^*$.
		3. Split into pseudo-control $X^*$ (first $n_1$) and pseudo-treatment $Y^*$ (last $n_2$).
		4. Compute $\theta^* = \bar{Y}^* - \bar{X}^*$.
	- P-value (two-sided): $$p=\frac{\sum^B_{b=1}I(|\theta^*_b| \geq |\theta|)}{B}$$
	Where $I$ is the indicator function.
	**Derivation Insight**:
	- The bootstrap assumes the empirical distribution approximates the true distribution.
	- Resampling mimics the sampling process under $H_0$ , where groups are drawn from the same population.
	- The distribution of $\theta^*$ estimates the variability of $\theta$, allowing p-value computation without parametric assumptions.
	**Proof Note**: No closed-form formula; consistency relies on the bootstrap principle: $n \to \infty$, the bootstrap distribution converges to the true sampling distribution (Efron, 1979).

5. **Permutation Test**
	**Purpose**: Tests differences by shuffling group labels to simulate the null hypothesis.
	**Formula/Algorithm**:
	- Observed statistic: $\theta = \bar{Y} - \bar{X}$.
	- For $M$ permutations:
	    1. Combine data: $Z = \{X_1, \dots, X_{n_1}, Y_1, \dots, Y_{n_2}\}$.
	    2. Randomly permute indices to assign $n_1$ to pseudo-control, $n_2$ to pseudo-treatment.
	    3. Compute $\theta^* = \bar{Y}^* - \bar{X}^*$
	- P-value (two-sided): $$p = \frac{\sum^M_{m=1}I(|\theta^*_m) \geq |\theta|}{M}$$
	**Derivation Insight**:
	- Under $H_0$​, group labels are exchangeable (no difference in distributions).
	- Permutations generate the null distribution of $\theta$.
	- The p-value measures how extreme the observed $\theta$ is in this distribution.
	**Proof Note**: Exact for finite samples (all permutations possible for small $n$). For large $n$, random permutations approximate the exact test (Fisher, 1935).

6. **Z-Test**
	**Purpose**: Compares means for large samples, assuming normality via the Central Limit Theorem.
	**Formula**: $$z = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{s_X^2}{n_1} + \frac{s_Y^2}{n_2}}}$$
	Where terms are as in Welch’s t-test.
	**P-Value**: Compare $z$ to the standard normal distribution: $p = 2 \cdot (1 - \Phi(|z|))$.
	Where $\Phi$ is the standard normal CDF.
	**Derivation Insight**:
	- For large $n_1, n_2, \bar{X}, \bar{Y}$ are approximately normal (CLT).
	- Variance of the difference is $\frac{\sigma_X^2}{n_1} + \frac{\sigma_Y^2}{n_2}$ estimated by $\frac{s_X^2}{n_1} + \frac{s_Y^2}{n_2}.$
	- Unlike the t-test, $s_X^2, s_Y^2$ are treated as close to $\sigma_X^2, \sigma_Y^2$​, $z \sim N(0,1)$ .
	**Proof Note**: The CLT ensures normality of $\bar{X} - \bar{Y}$, and Slutsky’s theorem justifies using sample variances.

7. **Kolmogorov-Smirnov (KS) Test**
	**Purpose**: Compares the entire distributions of two samples.
	**Formula**:
	- **Compute empirical CDFs:** $$F_X(x) = \frac{1}{n_1} \sum_{i=1}^{n_1} I(X_i \leq x), \quad F_Y(y) = \frac{1}{n_2} \sum_{i=1}^{n_2} I(Y_i \leq y)$$
	- **Test statistic:** $D = \sup_x |F_X(x) - F_Y(x)|$
	- For large samples, approximate p-value using: $$p \approx 2 \sum_{k=1}^\infty (-1)^{k-1} e^{-2k^2 n_1 n_2 D^2 / (n_1 + n_2)}$$
	**Derivation Insight**:
	- Under $H_0$​, $F_X = F_Y$​, so $D \approx 0$.
	- The statistic $D$ measures the maximum vertical distance between CDFs.
	- The limiting distribution of $\sqrt{\frac{n_1 n_2}{n_1 + n_2}} D$ is the Kolmogorov distribution.
	**Proof Note**: The asymptotic distribution is derived from Brownian bridge theory (Kolmogorov, 1933).

8. **Log-Transformed T-Test**
	**Purpose**: Applies Student’s t-test after log-transforming skewed data.
	**Formula**:
	- Transform data: $X_i' = \log(X_i + c), Y_i' = \log(Y_i + c)$ (where $c \geq -\min(X_i, Y_i)$ ensures positivity).
	- Apply Student’s t-test: $$t = \frac{\bar{X}' - \bar{Y}'}{s_p' \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$
	Where $\bar{X}', \bar{Y}', s_p'$  are computed on the transformed data.
	**Derivation Insight**:
	- For log-normal data, $X_i = e^{Z_i}​$, where $Z_i \sim N(\mu, \sigma^2)$.
	- Log-transform: $\log(X_i) = Z_i$​, which is normal, allowing a t-test.
	- The constant $c$ handles zeros/negatives but may affect interpretation.
	**Proof Note**: The t-test’s validity post-transformation relies on the transformed data meeting normality and equal-variance assumptions.
### Notes on Proofs
- **T-Tests and Z-Test**: Rely on normality and variance estimates, with proofs rooted in the t- and normal distributions.
- **Mann-Whitney U**: Derives from rank-sum distributions, with asymptotic normality for large samples.
- **Bootstrap and Permutation**: Lack closed-form proofs; their validity comes from resampling theory and exchangeability.
- **KS Test**: Uses stochastic process theory for its limiting distribution.
- **Log-Transform**: Assumes the transformation normalizes data, validated empirically.